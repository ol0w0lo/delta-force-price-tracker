import pathlib
from datetime import date
import csv

import pandas as pd
import requests
import streamlit as st
import altair as alt


# =========================
# CONFIG
# =========================
BACKFILL_FILE = pathlib.Path("delta_price_history_with_commit.csv")# generated by backfill_commit_time.py
LIVE_FILE = pathlib.Path("delta_price_live.csv")              # appended by this app button
SOURCE_URL = "https://raw.githubusercontent.com/orzice/DeltaForcePrice/master/price.json"

TIME_COL = "commit_time"   # universal timeline column


# =========================
# TIME / CLEANING HELPERS
# =========================
def _parse_is_get_time(series: pd.Series) -> pd.Series:
    """Parse is_get_time robustly (seconds/ms/iso strings)."""
    if series is None:
        return pd.Series(dtype="datetime64[ns, UTC]")

    s = series

    # numeric: decide seconds vs ms
    if pd.api.types.is_numeric_dtype(s):
        s_nonnull = s.dropna()
        if not s_nonnull.empty and (s_nonnull > 1e12).any():
            return pd.to_datetime(s, unit="ms", utc=True, errors="coerce")
        return pd.to_datetime(s, unit="s", utc=True, errors="coerce")

    # strings / mixed
    return pd.to_datetime(s, utc=True, errors="coerce")


def ensure_time_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Guarantee df has TIME_COL and it's parsed as UTC datetime.
    Priority:
      - commit_time (if exists and parseable)
      - fallback to is_get_time
    Also drops obviously invalid early dates (<2000).
    """
    df = df.copy()

    if "is_get_time" in df.columns:
        df["is_get_time"] = _parse_is_get_time(df["is_get_time"])

    if TIME_COL in df.columns:
        df[TIME_COL] = pd.to_datetime(df[TIME_COL], utc=True, errors="coerce")
    else:
        df[TIME_COL] = pd.NaT

    # fallback: fill commit_time from is_get_time
    if "is_get_time" in df.columns:
        df[TIME_COL] = df[TIME_COL].fillna(df["is_get_time"])

    # drop impossible early dates
    df.loc[df[TIME_COL] < pd.Timestamp("2000-01-01", tz="UTC"), TIME_COL] = pd.NaT

    if df[TIME_COL].isna().all():
        raise KeyError("No valid time values found in commit_time / is_get_time.")

    return df


def safe_date_range(df: pd.DataFrame) -> tuple[date, date]:
    """Return (min_date, max_date) based on TIME_COL, dropping NaT and absurd old values."""
    t = df[TIME_COL].dropna()
    t = t[t >= pd.Timestamp("2000-01-01", tz="UTC")]
    if t.empty:
        today = pd.Timestamp.utcnow().date()
        return today, today
    return t.min().date(), t.max().date()


def format_price(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return str(x)


# =========================
# IO FUNCTIONS
# =========================
def fetch_latest_snapshot() -> pd.DataFrame:
    """Fetch latest price.json from GitHub master."""
    r = requests.get(SOURCE_URL, timeout=20)
    r.raise_for_status()
    data = r.json()
    if not data:
        return pd.DataFrame()

    df = pd.DataFrame(data)

    # Ensure we have a timeline for new rows
    # master price.json usually has is_get_time; fallback to now if missing
    if "is_get_time" in df.columns:
        df = ensure_time_columns(df)
    else:
        df[TIME_COL] = pd.Timestamp.utcnow().tz_localize("UTC")

    return df


def append_to_live(df_new: pd.DataFrame) -> int:
    """Append latest snapshot rows into LIVE_FILE with safe quoting."""
    if df_new is None or df_new.empty:
        return 0

    df_new = ensure_time_columns(df_new)

    if LIVE_FILE.exists():
        df_new.to_csv(
            LIVE_FILE,
            mode="a",
            header=False,
            index=False,
            encoding="utf-8-sig",
            quoting=csv.QUOTE_ALL,
        )
    else:
        df_new.to_csv(
            LIVE_FILE,
            index=False,
            encoding="utf-8-sig",
            quoting=csv.QUOTE_ALL,
        )

    return len(df_new)


@st.cache_data
def load_merged_data() -> pd.DataFrame:
    """Load backfill + live files, merge, parse times, and return clean dataframe."""
    frames = []

    if BACKFILL_FILE.exists():
        df_b = pd.read_csv(BACKFILL_FILE, encoding="utf-8-sig", engine="python")
        if not df_b.empty:
            frames.append(df_b)

    if LIVE_FILE.exists():
        df_l = pd.read_csv(LIVE_FILE, encoding="utf-8-sig", engine="python")
        if not df_l.empty:
            frames.append(df_l)

    if not frames:
        return pd.DataFrame()

    df = pd.concat(frames, ignore_index=True)

    # Normalize time columns
    df = ensure_time_columns(df)

    # Basic cleanup
    if "name" in df.columns:
        df["name"] = df["name"].astype(str)

    # Keep only rows that have required fields
    required = {"name", "price", TIME_COL}
    for c in required:
        if c not in df.columns:
            raise KeyError(f"Missing required column: {c}")

    df = df.dropna(subset=["name", "price", TIME_COL])

    # Optional de-dupe if you have stable ids
    # if "id" in df.columns:
    #     df = df.drop_duplicates(subset=["id", TIME_COL], keep="last")

    return df


# =========================
# UI
# =========================
st.set_page_config(page_title="三角洲行动 价格分析", layout="wide")
st.title("三角洲行动 交易行价格分析工具")

st.write(
    "数据来源：\n"
    "- 历史回溯：`delta_price_backfill_only.csv`（GitHub 提交历史，含 commit_time）\n"
    "- 实时追加：`delta_price_live.csv`（点击左侧按钮获取最新快照）\n\n"
    "时间轴统一使用 `commit_time`。"
)

st.sidebar.header("数据控制")
st.sidebar.caption(f"历史文件: {BACKFILL_FILE.resolve() if BACKFILL_FILE.exists() else BACKFILL_FILE}")
st.sidebar.caption(f"实时文件: {LIVE_FILE.resolve() if LIVE_FILE.exists() else LIVE_FILE}")

if st.sidebar.button("立即抓取一次最新价格"):
    try:
        latest = fetch_latest_snapshot()
        n = append_to_live(latest)
        st.sidebar.success(f"抓取完成：新增 {n} 条记录。")
        load_merged_data.clear()
    except Exception as e:
        st.sidebar.error(f"抓取失败：{e}")

df = load_merged_data()

if df.empty:
    st.warning("没有可用数据。请先运行 backfill 生成历史文件，或点击左侧抓取最新价格。")
    st.stop()

# Sidebar filters
st.sidebar.subheader("筛选条件")

keyword = st.sidebar.text_input("物品名称包含：", value="", placeholder="例如：钢盔 / 5.56 / 狙击")

df_names = df
if keyword.strip():
    df_names = df[df["name"].str.contains(keyword.strip(), na=False)]

if df_names.empty:
    st.error("当前关键字下没有找到任何物品记录。请更换关键字。")
    st.stop()

all_names = sorted(df_names["name"].unique())
item_name = st.sidebar.selectbox("选择具体物品：", all_names)

# Date range defaults to selected item
global_min, global_max = safe_date_range(df)
tmp_item = df[df["name"] == item_name].copy()

item_min, item_max = safe_date_range(tmp_item)
item_min = max(item_min, global_min)
item_max = min(item_max, global_max)
if item_min > item_max:
    item_min, item_max = global_min, global_max

date_range = st.sidebar.date_input(
    "时间范围：",
    value=(item_min, item_max),
    min_value=global_min,
    max_value=global_max,
)

if isinstance(date_range, tuple):
    start_date, end_date = date_range
else:
    start_date, end_date = global_min, date_range

agg_mode = st.sidebar.selectbox("时间粒度：", ["原始数据", "按天平均"])

# Filter selected item
item_df = df[df["name"] == item_name].copy()
item_df = item_df.sort_values(TIME_COL)

mask = (item_df[TIME_COL].dt.date >= start_date) & (item_df[TIME_COL].dt.date <= end_date)
item_df = item_df[mask]

if item_df.empty:
    st.warning("该时间范围内没有记录，请调整时间范围。")
    st.stop()

# Aggregate daily
if agg_mode == "按天平均":
    item_df = (
        item_df.assign(_day=item_df[TIME_COL].dt.date)
        .groupby("_day", as_index=False)
        .agg({"price": "mean"})
    )
    item_df[TIME_COL] = pd.to_datetime(item_df["_day"], utc=True, errors="coerce")
    item_df = item_df.drop(columns=["_day"]).sort_values(TIME_COL)

# Layout
col_left, col_right = st.columns([2, 1])

with col_left:
    st.subheader(f"价格走势：{item_name}")

    chart = (
        alt.Chart(item_df)
        .mark_line(point=True)
        .encode(
            x=alt.X(f"{TIME_COL}:T", title="时间"),
            y=alt.Y("price:Q", title="价格"),
            tooltip=[
                alt.Tooltip(f"{TIME_COL}:T", title="时间"),
                alt.Tooltip("price:Q", title="价格"),
            ],
        )
        .interactive()
    )
    st.altair_chart(chart, use_container_width=True)

with col_right:
    st.subheader("统计信息")

    # 1) Convert price -> numeric (handles "$12.99", "12,999", "N/A", "", etc.)
    price_num = (
        item_df["price"]
        .astype(str)
        .str.strip()
        .str.replace(r"[\$,]", "", regex=True)
    )
    price_num = pd.to_numeric(price_num, errors="coerce")  # non-numeric -> NaN

    # 2) Sort and pick earliest/latest using numeric price (skip bad rows)
    item_sorted = item_df.assign(price_num=price_num).sort_values(TIME_COL)

    valid_sorted = item_sorted.dropna(subset=["price_num"])
    if valid_sorted.empty:
        st.metric("当前最新价格", "N/A")
        st.metric("时间段内最低价", "N/A")
        st.metric("时间段内最高价", "N/A")
        st.metric("时间段内平均价", "N/A")
        st.metric("价格波动（标准差）", "N/A")
        st.metric("时间段涨跌幅 (%)", "N/A")
        st.caption("提示：使用『按天平均』可以让长期趋势更清晰。")
    else:
        earliest_price = float(valid_sorted.iloc[0]["price_num"])
        latest_price   = float(valid_sorted.iloc[-1]["price_num"])

        price_min  = float(price_num.min(skipna=True))
        price_max  = float(price_num.max(skipna=True))
        price_mean = float(price_num.mean(skipna=True))
        price_std  = float(price_num.std(skipna=True))

        st.metric("当前最新价格", format_price(latest_price))
        st.metric("时间段内最低价", format_price(price_min))
        st.metric("时间段内最高价", format_price(price_max))
        st.metric("时间段内平均价", f"{price_mean:,.1f}")
        st.metric("价格波动（标准差）", f"{price_std:,.1f}" if pd.notna(price_std) else "N/A")

        if earliest_price != 0:
            pct = (latest_price / earliest_price - 1) * 100
            st.metric("时间段涨跌幅 (%)", f"{pct:,.1f}")
        else:
            st.metric("时间段涨跌幅 (%)", "N/A")

        st.caption("提示：使用『按天平均』可以让长期趋势更清晰。")


st.subheader("最近记录（原始数据）")
df_recent = df[df["name"] == item_name].sort_values(TIME_COL, ascending=False).head(50)
st.dataframe(df_recent.reset_index(drop=True))
